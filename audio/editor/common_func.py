import torch
import IPython.display
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch import optim
import torch.utils as utils
from torchvision import datasets, transforms
from sklearn.utils import resample
import librosa
import librosa.display
import os
import scipy.signal
import re
import sys
import pyaudio
from PIL import Image

__all__=["VAE",
         "slice_encode",
         "Points",]
class VAE(nn.Module):
    def __init__(self, x_dim, z_dim):
      super(VAE, self).__init__()
      self.x_dim = x_dim
      self.z_dim = z_dim
      self.fc1 = nn.Linear(x_dim, 20)
      self.fc2_mean = nn.Linear(20, z_dim)
      self.fc2_var = nn.Linear(20, z_dim)

      self.fc3 = nn.Linear(z_dim, 20)
      self.drop1 = nn.Dropout(p=0.2)
      self.fc4 = nn.Linear(20, x_dim)

    def encoder(self, x):
      x = x.view(-1, self.x_dim)
      x = F.relu(self.fc1(x))
      mean = self.fc2_mean(x)
      log_var = self.fc2_var(x)
      return mean, log_var

    def sample_z(self, mean, log_var, device):
      epsilon = torch.randn(mean.shape, device=device)
      return mean + epsilon * torch.exp(0.5*log_var)

    def decoder(self, z):
      y = F.relu(self.fc3(z))
      y = self.drop1(y)
      y = torch.sigmoid(self.fc4(y))
      return y

    def forward(self, x, device):
      x = x.view(-1, self.x_dim)
      mean, log_var = self.encoder(x)
      delta = 1e-8
      KL = 0.5 * torch.sum(1 + log_var - mean**2 - torch.exp(log_var))
      z = self.sample_z(mean, log_var, device)
      y = self.decoder(z)
      # 本来はmeanだがKLとのスケールを合わせるためにsumで対応
      reconstruction = torch.sum(x * torch.log(y + delta) + (1 - x) * torch.log(1 - y + delta))
      lower_bound = [KL, reconstruction]
      return -sum(lower_bound), z, y

class ExtentionException(Exception):
    pass

class EmptyLabelException(Exception):
    pass
class RecordData:
  def __init__(self,record_data):
    self.record_data =record_data

  def pop_extend(self,get_num):
    tmp=[]
    while len(len(tmp)>get_num):
      tmp.append(self.pop())
    np.array(tmp)
    np.flipud(tmp)
    tmp = tmp[len(tmp)-(get_num+1):]
    return np.concatenate(tmp, 0)

  def push(self,push_data):
    return self.record_data.append(push_data)

class Segment:
    """
    a unit of speech (i.e. phoneme, mora)
    """
    def __init__(self, tStart, tEnd, label):
        self.tStart = tStart
        self.tEnd = tEnd
        self.label = label

    def __add__(self, other):
        return Segment(self.tStart, other.tEnd, self.label + other.label)

    def can_follow(self, other):
        """
        return True if Segment self can follow Segment other in one mora,
        otherwise return False
        example: (other, self)
             True: ('s', 'a'), ('sh', 'i'), ('ky', 'o:'), ('t', 's')
             False: ('a', 'q'), ('a', 's'), ('u', 'e'), ('s', 'ha')
        """
        vowels = ['a', 'i', 'u', 'e', 'o', 'a:', 'i:', 'u:', 'e:', 'o:']
        consonants = ['w', 'r', 't', 'y', 'p', 's', 'd', 'f', 'g', 'h', 'j',
                      'k', 'z', 'c', 'b', 'n', 'm']
        only_consonants = lambda x: all([c in consonants for c in x])
        if only_consonants(other.label) and self.label in vowels:
            return True
        if only_consonants(other.label) and only_consonants(self.label):
            return True
        return False

def read_lab(filename):
    """
    read label file (.lab) generated by Julius segmentation kit and
    return SegmentationLabel object
    """
    try:
        if not re.search(r'\.lab$', filename):
            raise ExtentionException("read_lab supports only .lab")
    except ExtentionException as e:
        print(e)
        return None

    with open(filename, 'r') as f:
        labeldata = [line.split() for line in f if line != '']
        segments = [Segment(tStart=float(line[0]), tEnd=float(line[1]),
                            label=line[2])
                    for line in labeldata]
        return SegmentationLabel(segments)

class SegmentationLabel:
    """
    list of segments
    """
    def __init__(self, segments, separatedByMora=False):
        self.segments = segments
        self.separatedByMora = separatedByMora
    def by_moras(self):
        """
        return new SegmentationLabel object whose segment are moras
        """
        if self.separatedByMora == True:
            return self

        moraSegments = []
        curMoraSegment = None
        for segment in self.segments:
            if curMoraSegment is None:
                curMoraSegment = segment
            elif segment.can_follow(curMoraSegment):
                curMoraSegment += segment
            else:
                moraSegments.append(curMoraSegment)
                curMoraSegment = segment
        if curMoraSegment:
            moraSegments.append(curMoraSegment)
        return SegmentationLabel(moraSegments, separatedByMora=True)
    
class Point:
  PREDICT_ALGORITHM = None
  def __init__(self,z,time_start=None,time_end=None,predicted_label=None):
      
      self.z=[]
      for i in z:
         self.z.append(i)
      self.time_start = time_start
      self.time_end = time_end
      self.predicted_label = predicted_label

  def add(self,other):
     return Point(self.z+other.z,self.time_start,other.time_end,predicted_label=self.predicted_label)

  def label_can_follow(self,other):
    if self.predicted_label==other.predicted_label:
       return True
    else :
       return False
     
  def predict(self,predict_algorithm=None):
    if predict_algorithm == None:
       predict_algorithm = Point.PREDICT_ALGORITHM
    elif predict_algorithm == None and Point.PREDICT_ALGORITHM == None:
       return None
    
    predict = predict_algorithm.predict(self.z)
    self.predicted_label = predict[0]

    return predict[0]
  

  def get_z(self):
     return self.z
  

class Points:
  def __init__(self,points:Point,point_sorted_label=False):
      self.points = points
      self.point_sorted_label = point_sorted_label


  def point_sort(self):
    cur_point:Point = None
    next_points = []
    
    for point in self.points:
        if cur_point == None:
          cur_point = point
        elif cur_point.label_can_follow(point):
          cur_point = cur_point.add(point)
        else :
          next_points.append(cur_point)
          cur_point = point
    next_points.append(cur_point)
    return Points(next_points,point_sorted_label=True)




class visualizer2D:
  def __init__(self,x_lim=[-3,3],y_lim=[-3,3],fig_size=[10,10],image_path="z.png"):
    self.fig_anm = plt.figure(figsize=(fig_size[0],fig_size[1]))
    self.ax=self.fig_anm.add_subplot(1,1,1)
    self.image = Image.open(image_path)
    self.ax.set_xlim(x_lim[0],x_lim[1])
    self.ax.set_ylim(y_lim[0],y_lim[1])
    self.xlim = self.ax.get_xlim()
    self.ylim = self.ax.get_ylim()
  
  def visual(self,x,y):
    self.ax.scatter(x,y, c="pink", alpha=1, linewidths=2,edgecolors="red")
    self.ax.imshow(self.image,extent=[*self.xlim,*self.ylim], aspect='auto',alpha=0.6) 
    plt.draw()
    plt.pause(0.1)
    plt.cla()      
    
def read_wave_in_jvs(wave_path,label_path,sr,time_span=800,threshold=0.1,target=['a','i','u','e','o','a:','i:','u:','e:','o:']):
  wave_data, _ = librosa.load(wave_path, sr=sr)
  label = read_lab(label_path)

  input_data = []
  input_label_data = []
  for seg in label.segments:
    if seg.label in target:
      start = int(seg.tStart*sr)
      end = int(seg.tEnd*sr)
      if (end - start) <= time_span:
        continue
      tmp_wave = wave_data[start:end]
      wavelen = len(tmp_wave)
      for i in range(0,wavelen//time_span):
        tmp = tmp_wave[time_span*i:time_span*(i+1)]#0.05秒
        tmp = np.array(tmp)
        if np.max(tmp)>threshold:
          input_data.append(tmp)
          input_label_data.append(seg.label)


  return np.array(input_data,dtype=object),np.array(input_label_data,dtype=object)

def read_jvs(folder_num,sr,time_span=800,threshold=0.1,target=['a','i','u','e','o','a:','i:','u:','e:','o:']):
  folder_path = 'data'
  wave_folder_path = os.path.join(folder_path,f"jvs{folder_num:03d}\parallel100\wav24kHz16bit")
  label_folder_path = os.path.join(folder_path,f"jvs{folder_num:03d}\parallel100\lab\mon")
  input_data=[]

  input_label_data=[]

  for i in range(1,101):
    filename = f"VOICEACTRESS100_{i:03d}"
    wave_path = os.path.join(wave_folder_path,filename+".wav")
    label_path = os.path.join(label_folder_path,filename+".lab")
    tmp_data,tmp_label=read_wave_in_jvs(wave_path,label_path,sr,time_span,threshold,target=target)
    if(tmp_data.shape[0]!=0):
      input_data.append(tmp_data)
      input_label_data.append(tmp_label)
  input_data = np.concatenate(input_data)
  input_label_data = np.concatenate(input_label_data)
  return np.array(input_data,dtype=object),np.array(input_label_data,dtype=object)

def cep(audio,frame_len=512,hop_len=256,dim=50):
  # 窓関数の設定
  window = np.hamming(frame_len)
  # フレームごとにケプストラム解析を行う
  log_data=[]
  cepstrum_data =[]
  origin_data = []
  for wave in audio:
    cepstrum = []
    tmp = []
    log_tmp=[]
    for i in range(0, len(wave)-hop_len, hop_len):
      if i+frame_len > len(wave):
        break
      # 窓関数をかける
      frame = wave[i:i+frame_len] * window
      tmp.append(frame)
      # フーリエ変換
      spectrum = np.fft.fft(frame)
      # 対数振幅スペクトル
      log_spectrum = np.log(np.abs(spectrum))
      log_tmp.append(log_spectrum)
      # 逆フーリエ変換
      ceps = np.fft.ifft(log_spectrum)
      ceps[dim:len(ceps)-dim]=0
      # ケプストラムのリストに追加
      cepstrum.append(ceps)
    log_data.append(log_tmp)
    origin_data.append(tmp)
    cepstrum_data.append(cepstrum)
  # ケプストラムの配列に変換
  cepstrum_data = np.array(cepstrum_data)
  # 声道スペクトルの抽出
  vocal_tract_spectrum = np.exp(np.real(cepstrum_data))
  origin_data = np.array(origin_data)
  log_data=np.array(log_data)
  print(log_data.shape)
  print(vocal_tract_spectrum.shape)
  print(origin_data.shape)

  # 声道スペクトルからケプストラムへ変換
  tmp = np.log(vocal_tract_spectrum)

  # ケプストラムから対数振幅スペクトルへ変換
  log_spectrum = np.fft.fft(tmp)
  half_index = log_spectrum.shape[2]//2
  log_spectrum = log_spectrum.real[:,:,0:half_index]
  print(log_spectrum.shape)
  fo = []
  fo_onehot = []
  for i,wave_data in enumerate(log_spectrum):
    tmp = []
    fo_tmp = []
    for j,data in enumerate(wave_data):
      fo_tmp2 = np.zeros(256)
      max_index = scipy.signal.argrelmax(data, order=3)
      max_index = max_index[0]
      tmp.append(max_index)
      for k in range(4):
        if 0<=k and k<len(max_index):
          fo_tmp2[max_index[k]]=1
      fo_tmp.append(fo_tmp2)
    fo_onehot.append(fo_tmp)
    fo.append(tmp)
  fo = np.array(fo)
  fo_onehot = np.array(fo_onehot)
  fig = plt.figure(figsize=(10,6))
  ax = fig.add_subplot(1, 1, 1)
  frame = np.random.randint(0,log_spectrum.shape[0])
  ax.plot(log_spectrum[frame][0][0:log_spectrum.shape[2]])
  ax.axvline(fo[frame][0][0], ls = "--", color = "navy")
  ax.axvline(fo[frame][0][1], ls = "--", color = "navy")
  ax.axvline(fo[frame][0][2], ls = "--", color = "navy")
  ax.axvline(fo[frame][0][3], ls = "--", color = "navy")
  plt.xlabel('Time')
  plt.ylabel('Amplitude')
  plt.title(f'Waveform of frame {frame}')
  plt.show()
  plt.figure(figsize=(10, 6))
  plt.plot(log_data[frame][0][0:log_spectrum.shape[2]])
  plt.show()
  return log_spectrum,fo_onehot,fo

def normal(x):

  x_scaled = (x-x.min())/(x.max()-x.min()+1e-5)

  return x_scaled

def realtime_recording(sr):
    CHUNK = 1024  # 音声データのチャンクサイズ
    FORMAT = pyaudio.paInt16  # 音声データのフォーマット
    CHANNELS = 1  # モノラル
    RATE = sr  # サンプリングレート

    p = pyaudio.PyAudio()

    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    output=True,
                    frames_per_buffer=CHUNK)
    return p, stream
def stop_recording(audio, stream):
    stream.stop_stream()
    stream.close()
    audio.terminate()

def output_data(sr,n_fft,hop_length,n_mels,frame_len,dataset_mean,dataset_std,model,device,stream=None,input_data=None,standard_flag=False,input_type_stream=True,threshold=0):
   
    if not input_type_stream:
       audio_data = input_data
    else:
      audio_data = stream.read(frame_len)
      audio_data = np.frombuffer(audio_data, dtype='int16')/32768
    
    if np.abs(np.max(audio_data))>threshold:
      mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels,center=False)
      log_mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)
      if standard_flag:
        l2 = log_mel_spec - dataset_mean
        log_mel_spec = l2/dataset_std
        
      np_meldata = normal(log_mel_spec)
      tensor_meldata =torch.from_numpy(np_meldata.astype(np.float32)).clone()
      x = tensor_meldata.to(device)
      tmp,z,y=model(x,device)
      z = z.cpu().detach().numpy()
      z = z[0] 
      z = Point([z])
      return z
    else:
       z=Point(z=None)
    
def output_data_batch_normalization(stream,sr,n_fft,hop_length,n_mels,frame_len,dataset_mean,dataset_std,model,device):
    audio_data = stream.read(frame_len),
    audio_data = np.frombuffer(audio_data, dtype='int16')/32768
    mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels,center=False)
    log_mel_spec = librosa.amplitude_to_db(mel_spec, ref=np.max)
    np_meldata = normal(log_mel_spec)
    tensor_meldata =torch.from_numpy(np_meldata.astype(np.float32)).clone()
    x = tensor_meldata.to(device)
    tmp,z,y=model(x,device)
    z = z.cpu().detach().numpy()
    z = z[0] 
    return z

def slice_encode(sr,n_fft,hop_length,n_mels,frame_len,dataset_mean,dataset_std,model,device,input_data=None,threshold=0):
  num_frame = len(input_data)//frame_len
  input_data = input_data[:num_frame*frame_len]
  print(input_data.shape)
  latent = []
  for i in range(0,num_frame):
    data = input_data[i*frame_len:(i+1)*frame_len]
    z = output_data(sr,n_fft,hop_length,n_mels,frame_len,dataset_mean,
                             dataset_std,model,device,stream=None,input_data=data,
                             standard_flag=False,input_type_stream=False,threshold=0)
    z.time_start = i*frame_len
    z.time_end = (i+1)*frame_len
    latent.append(z)


  return latent

